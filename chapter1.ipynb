{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as num\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtransforms\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import statistics as st\n",
    "from scipy import integrate as intg\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from bioinfokit.analys import stat\n",
    "import os\n",
    "from statsmodels.graphics.factorplots import interaction_plot\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'Chapter1_three_bin'\n",
    "growingseasonsdf = pd.read_csv(\"growing_seasons_all.csv\")\n",
    "dailydf = pd.read_csv(\"AMF_US-MOz_FLUXNET_SUBSET_DD_2004-2019_3-5.csv\")\n",
    "halfhourlydf = pd.read_csv(\"AMF_US-MOz_FLUXNET_SUBSET_HH_2004-2019_3-5.csv\")\n",
    "halfhourlydf = halfhourlydf.replace(-9999, num.NaN)\n",
    "\n",
    "#inner joining growingseasondf and dailydf to isolate growing seasons\n",
    "growingseasonsdailydf = pd.merge(growingseasonsdf, dailydf, on='TIMESTAMP')\n",
    "\n",
    "#empty column for did it rain? your name and stress lvl flags\n",
    "growingseasonsdailydf['PRECIP_YN'] = num.nan\n",
    "growingseasonsdailydf['STRESS_LVL'] = num.nan\n",
    "\n",
    "#function for first day of growing seasons\n",
    "def first_day_precip(df: pd.DataFrame, ind):\n",
    "    if df.at[ind, 'P_F']>=5:\n",
    "        df.at[ind, 'PRECIP_YN']= 'Wet'\n",
    "    else:\n",
    "        df.at[ind, 'PRECIP_YN']= 'Dry'\n",
    "\n",
    "for ind in growingseasonsdailydf.index:\n",
    "    #precip condition flagging\n",
    "    #first day of entire growingseasondailydf\n",
    "    if ind == 0:\n",
    "        first_day_precip(growingseasonsdailydf, ind)\n",
    "    else:\n",
    "        counter=growingseasonsdailydf.at[ind-1, 'index']\n",
    "        prevrain=growingseasonsdailydf.at[ind-1, 'P_F']\n",
    "        #check to see if in same year\n",
    "        if growingseasonsdailydf.at[ind, 'index']>counter:\n",
    "            #if same year, mark precip_yn\n",
    "            if growingseasonsdailydf.at[ind, 'P_F']>=5 and prevrain>=5:\n",
    "                growingseasonsdailydf.at[ind, 'PRECIP_YN']='wWet'\n",
    "            elif growingseasonsdailydf.at[ind, 'P_F']>=5 and prevrain<5:\n",
    "                growingseasonsdailydf.at[ind, 'PRECIP_YN']='dWet'\n",
    "            elif growingseasonsdailydf.at[ind, 'P_F']<5 and prevrain>=5:\n",
    "                growingseasonsdailydf.at[ind, 'PRECIP_YN']='wDry'\n",
    "            elif growingseasonsdailydf.at[ind, 'P_F']<5 and prevrain<5:\n",
    "                growingseasonsdailydf.at[ind, 'PRECIP_YN']='dDry'\n",
    "            else:\n",
    "                print(ind)\n",
    "        #if not in same year, don't count day before\n",
    "        else:\n",
    "            first_day_precip(growingseasonsdailydf, ind)\n",
    "\n",
    "    #stress level flagging\n",
    "    if growingseasonsdailydf.at[ind, ' PLWP']<0 and growingseasonsdailydf.at[ind, ' PLWP']>= -0.5:\n",
    "        growingseasonsdailydf.at[ind, 'STRESS_LVL']='Low'\n",
    "    elif growingseasonsdailydf.at[ind, ' PLWP']< -0.5 and growingseasonsdailydf.at[ind, ' PLWP']>= -1:\n",
    "        growingseasonsdailydf.at[ind, 'STRESS_LVL']='Medium'\n",
    "    elif growingseasonsdailydf.at[ind, ' PLWP']< -1:\n",
    "        growingseasonsdailydf.at[ind, 'STRESS_LVL']='High'\n",
    "    else:\n",
    "        print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty lists for each bin i.e. wWet AND mediumLow\n",
    "dDryLow = []\n",
    "dDryMedium = []\n",
    "dDryHigh = []\n",
    "\n",
    "wDryLow = []\n",
    "wDryMedium = []\n",
    "wDryHigh = []\n",
    "\n",
    "dWetLow = []\n",
    "dWetMedium =[]\n",
    "dWetHigh = []\n",
    "\n",
    "wWetLow =[]\n",
    "wWetMedium = []\n",
    "wWetHigh = []\n",
    "\n",
    "#sort timestamps into respective bins\n",
    "for ind in growingseasonsdailydf.index:\n",
    "    if growingseasonsdailydf.at[ind, 'PRECIP_YN']=='dDry' and growingseasonsdailydf.at[ind, 'STRESS_LVL']=='Low':\n",
    "        dDryLow.append(str(growingseasonsdailydf.at[ind, 'TIMESTAMP']))\n",
    "    elif growingseasonsdailydf.at[ind, 'PRECIP_YN']=='dDry' and growingseasonsdailydf.at[ind, 'STRESS_LVL']=='Medium':\n",
    "        dDryMedium.append(str(growingseasonsdailydf.at[ind, 'TIMESTAMP']))\n",
    "    elif growingseasonsdailydf.at[ind, 'PRECIP_YN']=='dDry' and growingseasonsdailydf.at[ind, 'STRESS_LVL']=='High':\n",
    "        dDryHigh.append(str(growingseasonsdailydf.at[ind, 'TIMESTAMP']))\n",
    "\n",
    "    elif growingseasonsdailydf.at[ind, 'PRECIP_YN']=='wDry' and growingseasonsdailydf.at[ind, 'STRESS_LVL']=='Low':\n",
    "        wDryLow.append(str(growingseasonsdailydf.at[ind, 'TIMESTAMP']))\n",
    "    elif growingseasonsdailydf.at[ind, 'PRECIP_YN']=='wDry' and growingseasonsdailydf.at[ind, 'STRESS_LVL']=='Medium':\n",
    "        wDryMedium.append(str(growingseasonsdailydf.at[ind, 'TIMESTAMP']))\n",
    "    elif growingseasonsdailydf.at[ind, 'PRECIP_YN']=='wDry' and growingseasonsdailydf.at[ind, 'STRESS_LVL']=='High':\n",
    "        wDryHigh.append(str(growingseasonsdailydf.at[ind, 'TIMESTAMP']))\n",
    "\n",
    "    elif growingseasonsdailydf.at[ind, 'PRECIP_YN']=='dWet' and growingseasonsdailydf.at[ind, 'STRESS_LVL']=='Low':\n",
    "        dWetLow.append(str(growingseasonsdailydf.at[ind, 'TIMESTAMP']))\n",
    "    elif growingseasonsdailydf.at[ind, 'PRECIP_YN']=='dWet' and growingseasonsdailydf.at[ind, 'STRESS_LVL']=='Medium':\n",
    "        dWetMedium.append(str(growingseasonsdailydf.at[ind, 'TIMESTAMP']))\n",
    "    elif growingseasonsdailydf.at[ind, 'PRECIP_YN']=='dWet' and growingseasonsdailydf.at[ind, 'STRESS_LVL']=='High':\n",
    "        dWetHigh.append(str(growingseasonsdailydf.at[ind, 'TIMESTAMP']))\n",
    "\n",
    "    elif growingseasonsdailydf.at[ind, 'PRECIP_YN']=='wWet' and growingseasonsdailydf.at[ind, 'STRESS_LVL']=='Low':\n",
    "        wWetLow.append(str(growingseasonsdailydf.at[ind, 'TIMESTAMP']))\n",
    "    elif growingseasonsdailydf.at[ind, 'PRECIP_YN']=='wWet' and growingseasonsdailydf.at[ind, 'STRESS_LVL']=='Medium':\n",
    "        wWetMedium.append(str(growingseasonsdailydf.at[ind, 'TIMESTAMP']))\n",
    "    elif growingseasonsdailydf.at[ind, 'PRECIP_YN']=='wWet' and growingseasonsdailydf.at[ind, 'STRESS_LVL']=='High':\n",
    "        wWetHigh.append(str(growingseasonsdailydf.at[ind, 'TIMESTAMP']))\n",
    "    else:\n",
    "        #going to default to this on the first days of the growing seasons where it only has wet or dry\n",
    "        continue\n",
    "\n",
    "#create new column in halfhourly df to inner join\n",
    "halfhourlydf = halfhourlydf.astype({'TIMESTAMP_START':'string'})\n",
    "halfhourlydf['TIMESTAMP']=halfhourlydf['TIMESTAMP_START'].str[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_eWUE(df: pd.DataFrame):\n",
    "    df['lambda'] = (2.501 - (df['TA_F'] * 0.002361))*1000000\n",
    "    df['ET'] = df['LE_F_MDS'] / df['lambda']\n",
    "    df['GPP_NT_VUT_50'] = df['GPP_NT_VUT_50'] * 0.001\n",
    "    df['eWUE'] = df['GPP_NT_VUT_50'] / (df['ET']) / 1000 * 18.02\n",
    "def calc_LUE(df: pd.DataFrame):\n",
    "    # df['PAR'] = df['SW_IN_F'] / 2 / 235000\n",
    "    df['PAR'] = df['PPFD_IN'] - df['PPFD_OUT']\n",
    "    df['LUE'] = df['GPP_NT_VUT_50'] / df['PAR'] * 1000\n",
    "\n",
    "#if nighttime, set GPP to nan <--- we only want to calculate eWUE and LUE for daytime values\n",
    "halfhourlydf.loc[halfhourlydf['SW_IN_F'] <=100, 'GPP_NT_VUT_50'] = num.nan\n",
    "#create columns to hold only daytime values for LE and H for future EF calculations\n",
    "halfhourlydf['LE_DAYTIME']=halfhourlydf['LE_F_MDS']\n",
    "halfhourlydf['H_DAYTIME']=halfhourlydf['H_F_MDS']\n",
    "halfhourlydf.loc[halfhourlydf['SW_IN_F'] <=5, 'LE_DAYTIME'] = num.nan\n",
    "halfhourlydf.loc[halfhourlydf['SW_IN_F'] <=5, 'H_DAYTIME'] = num.nan\n",
    "#if LE is = 0 then set to nan, LE shouldn't be 0 and is only zero because it is gap filled, so this avoids that \n",
    "halfhourlydf.loc[halfhourlydf['LE_F_MDS']==0, 'LE_F_MDS'] = num.nan\n",
    "halfhourlydf.loc[halfhourlydf['H_F_MDS']==0, 'H_F_MDS'] = num.nan\n",
    "\n",
    "calc_eWUE(halfhourlydf)\n",
    "calc_LUE(halfhourlydf)\n",
    "#reasonable range filters for eWUE and LUE\n",
    "halfhourlydf.loc[halfhourlydf['eWUE']>=40, 'eWUE'] = num.nan\n",
    "halfhourlydf.loc[halfhourlydf['eWUE']<= 0, 'eWUE'] = num.nan\n",
    "# halfhourlydf.loc[halfhourlydf['LUE']> 0.1, 'LUE'] = num.nan\n",
    "halfhourlydf.loc[halfhourlydf['LUE']< 0, 'LUE'] = num.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty parent lists to put stats lists into\n",
    "eflistforanova = []\n",
    "nee_min = []\n",
    "nee_max = []\n",
    "nee_int = []\n",
    "le_max = []\n",
    "le_int = []\n",
    "h_max = []\n",
    "h_int = []\n",
    "netrad_max = []\n",
    "netrad_int = []\n",
    "centroid_ = []\n",
    "\n",
    "#create empty dfs to put in stats\n",
    "needf = pd.DataFrame(columns=['BIN', 'MIN', 'MIN_STD', 'MAX', 'MAX_STD','INT', 'INT_STD'])\n",
    "ledf = pd.DataFrame(columns=['BIN', 'MIN', 'MIN_STD', 'MAX', 'MAX_STD', 'INT', 'INT_STD'])\n",
    "hdf = pd.DataFrame(columns=['BIN', 'MIN', 'MIN_STD', 'MAX', 'MAX_STD', 'INT', 'INT_STD'])\n",
    "netraddf = pd.DataFrame(columns=['BIN', 'MIN', 'MIN_STD', 'MAX', 'MAX_STD', 'INT', 'INT_STD'])\n",
    "efdf = pd.DataFrame(columns=['BIN', 'MEAN', 'STD_DEV'])\n",
    "centroiddf = pd.DataFrame(columns=['BIN', 'Time', 'SE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take variables and put into matrix so that columns are days and rows are half hours\n",
    "def variable_matrix(df: pd.DataFrame, variable: str, n):\n",
    "    varlist = df[variable].values.tolist()\n",
    "    varlist = num.reshape(varlist, (n, 48))\n",
    "    varlist = num.transpose(varlist)\n",
    "    return varlist\n",
    "\n",
    "#weighted centroid function, taken from Nelson et al. 2018\n",
    "def diurnal_centroid(flux, bin, unitesperday=48):\n",
    "    days, UPD = flux.reshape(-1,unitesperday).shape\n",
    "    hours = num.tile(num.arange(UPD),days).reshape(days,UPD)\n",
    "    C=num.sum(hours*flux.reshape(-1, 48), axis=1)/num.sum(flux.reshape(-1,48), axis=1)\n",
    "    C = C*(24/unitesperday)\n",
    "    C, SE = mean_stddev(C, days)\n",
    "    cenlist = [bin, C, SE]\n",
    "    centroiddf.loc[len(centroiddf.index)] = cenlist\n",
    "\n",
    "#spitting out SE not SD anymore\n",
    "def mean_stddev(list, n):\n",
    "    mean = num.nanmean(list)\n",
    "    std_dev = num.nanstd(list)\n",
    "    std_err = std_dev / num.sqrt(n)\n",
    "    return mean, std_err\n",
    "\n",
    "def variable_stats(list, n, bin: str, var:str):\n",
    "    #create lists with stats from every day\n",
    "    minlist = []\n",
    "    maxlist = []\n",
    "    intlist = []\n",
    "    #centroidlist = []\n",
    "    # for each column, aka, for each day within bin\n",
    "    for i in range(n):\n",
    "        col = list[:, i]\n",
    "        daytime_col = list[12:36, i]\n",
    "        if num.isnan(daytime_col).all():\n",
    "            continue\n",
    "        else:\n",
    "            mini = num.nanmin(col)\n",
    "            maxi = num.nanmax(col)\n",
    "            minlist.append(mini)\n",
    "            maxlist.append(maxi)\n",
    "            integral = [x*1800 for x in col]\n",
    "            intlist.append(sum(integral))\n",
    "        # if var == 'nee':\n",
    "        #     #centroid function\n",
    "        #     hours = num.arange(0, 48)\n",
    "        #     C=num.sum(hours*col)/num.sum(col)\n",
    "        #     C = C*(24/48)\n",
    "        #     centroidlist.append(C)\n",
    "   \n",
    "    #put stats lists into parent lists to save for later for anova\n",
    "    if var == 'nee':\n",
    "        # centroid_.append(centroidlist)\n",
    "        # cen_mean, cen_std = mean_stddev(centroidlist, n)\n",
    "        # cenlist = [bin, cen_mean, cen_std]\n",
    "        # centroiddf.loc[len(centroiddf.index)] = cenlist\n",
    "\n",
    "        nee_min.append(minlist)\n",
    "        intlist = [x/1000 for x in intlist]\n",
    "        nee_int.append(intlist)\n",
    "        nee_max.append(maxlist)\n",
    "    elif var == 'le':\n",
    "        le_max.append(maxlist)\n",
    "        intlist = [x/1000000 for x in intlist]\n",
    "        le_int.append(intlist)\n",
    "    elif var == 'h':\n",
    "        h_max.append(maxlist)\n",
    "        intlist = [x/1000000 for x in intlist]\n",
    "        h_int.append(intlist)\n",
    "    elif var == 'netrad':\n",
    "        netrad_max.append(maxlist)\n",
    "        intlist = [x/1000000 for x in intlist]\n",
    "        netrad_int.append(intlist)\n",
    "    else:\n",
    "        print('invaild var: str input in variable_stats')\n",
    "    \n",
    "    #find average of those lists, all SDs are now SEs\n",
    "    min_mean, min_std = mean_stddev(minlist, n)\n",
    "    max_mean, max_std = mean_stddev(maxlist, n)\n",
    "    int_mean, int_std = mean_stddev(intlist, n)\n",
    "    #return list with all those stats\n",
    "    statslist = [bin, min_mean, min_std, max_mean, max_std, int_mean, int_std] \n",
    "    return statslist\n",
    "\n",
    "def statistic_calcs(df: pd.DataFrame, n, bin: str):\n",
    "    # df is ddl, ddm, ddh\n",
    "    # n is number of days \n",
    "    # bin is a string 'ddl', 'ddm', 'ddh'\n",
    "    neelist = variable_matrix(df, 'NEE_VUT_REF', n)\n",
    "    neelist = variable_stats(neelist, n, bin, 'nee')\n",
    "    needf.loc[len(needf.index)] = neelist\n",
    "\n",
    "    lelist = variable_matrix(df, 'LE_F_MDS', n)\n",
    "    lelist = variable_stats(lelist, n, bin, 'le')\n",
    "    ledf.loc[len(ledf.index)] = lelist\n",
    "\n",
    "    hlist = variable_matrix(df, 'H_F_MDS', n)\n",
    "    hlist = variable_stats(hlist, n, bin, 'h')\n",
    "    hdf.loc[len(hdf.index)]=hlist\n",
    "\n",
    "    netradlist = variable_matrix(df, 'NETRAD', n)\n",
    "    netradlist = variable_stats(netradlist, n, bin, 'netrad')\n",
    "    netraddf.loc[len(netraddf.index)]=netradlist\n",
    "\n",
    "def efints(criteria:pd.DataFrame, bin:str, n):\n",
    "    #ef calculation with LE an H daytime integrals\n",
    "    LElist = variable_matrix(criteria, 'LE_DAYTIME', n)\n",
    "    Hlist = variable_matrix(criteria, 'H_DAYTIME', n)\n",
    "    LEintlist = []\n",
    "    Hintlist = []\n",
    "    for i in range(n):\n",
    "        daytimeH = Hlist[:, i]\n",
    "        daytimeH = [x for x in daytimeH if ~num.isnan(x) == True]\n",
    "        daytimeLE = LElist[:, i]\n",
    "        daytimeLE = [x for x in daytimeLE if ~num.isnan(x) == True]\n",
    "\n",
    "        # xdata = num.sum(~num.isnan(Hlist[:,i]))\n",
    "        # xdataH = num.arange(0, xdata)\n",
    "        # xdataLE = num.sum(~num.isnan(LElist[:,i]))\n",
    "        # xdataLE = num.arange(0, xdataLE)\n",
    "        # LEint = intg.trapz(daytimeLE, xdataLE)\n",
    "        # Hint = intg.trapz(daytimeH, xdataH)\n",
    "\n",
    "        LEint = [x*1800 for x in daytimeLE]\n",
    "        LEint = sum(LEint)\n",
    "        Hint = [x*1800 for x in daytimeH]\n",
    "        Hint = sum(Hint)\n",
    "        LEint = round(LEint, 4)\n",
    "        Hint = round(Hint, 4)\n",
    "        LEintlist.append(LEint)\n",
    "        Hintlist.append(Hint)\n",
    "    # keeping ints stored in case need for something else\n",
    "    efintsdf = pd.DataFrame({'LEint': LEintlist, 'Hint': Hintlist})\n",
    "    #calculate EF\n",
    "    efintsdf['EF'] = efintsdf['LEint'] / (efintsdf['LEint'] + efintsdf['Hint'])\n",
    "    eflist = efintsdf['EF'].values.tolist()\n",
    "    eflistforanova.append(eflist)\n",
    "    ef_mean, ef_std = mean_stddev(eflist, n)\n",
    "    eflist = [bin, ef_mean, ef_std]\n",
    "    efdf.loc[len(efdf.index)]= eflist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_taunc(df: pd.DataFrame):\n",
    "    df['TA_INSTUNC']=None\n",
    "    df['RH_INSTUNC']=None\n",
    "    df['PA_INSTUNC']=None\n",
    "    for ind in df.index:\n",
    "        if df.loc[ind, 'TA_F'] > 20:\n",
    "            df.loc[ind, 'TA_INSTUNC'] = 0.055 + 0.0057 * df.loc[ind, 'TA_F']\n",
    "        else:\n",
    "            df.loc[ind, 'TA_INSTUNC'] = 0.226 - 0.0028 * df.loc[ind, 'TA_F']\n",
    "        \n",
    "        if df.loc[ind, 'TA_F'] < -40:\n",
    "            df.loc[ind, 'RH_INSTUNC'] = 1.4 + 0.032*df.loc[ind, 'RH'] \n",
    "        elif df.loc[ind, 'TA_F'] < -20:\n",
    "            df.loc[ind, 'RH_INSTUNC'] = 1.2 + 0.012*df.loc[ind, 'RH']\n",
    "        elif df.loc[ind, 'TA_F'] < 40:\n",
    "            df.loc[ind, 'RH_INSTUNC'] = 1 + 0.008*df.loc[ind, 'RH']\n",
    "        elif df.loc[ind, 'TA_F'] < 60:\n",
    "            df.loc[ind, 'RH_INSTUNC'] = 1.2 + 0.012*df.loc[ind, 'RH']\n",
    "\n",
    "        if df.loc[ind, 'TA_F'] >= 15 and df.loc[ind, 'TA_F'] <= 25:\n",
    "            if df.loc[ind, 'RH'] < 90:\n",
    "                df.loc[ind, 'RH_INSTUNC'] = 1\n",
    "            else: df.loc[ind, 'RH_INSTUNC'] = 1.7\n",
    "        \n",
    "        if 15 <= df.loc[ind, 'TA_F'] <= 25:\n",
    "            df.loc[ind, 'PA_INSTUNC'] = 0.5\n",
    "        elif 0 <= df.loc[ind, 'TA_F'] <= 40:\n",
    "            df.loc[ind, 'PA_INSTUNC'] = 1\n",
    "        elif -20 <= df.loc[ind, 'TA_F'] <= 50:\n",
    "            df.loc[ind, 'PA_INSTUNC'] = 1.5\n",
    "        elif -40 <= df.loc[ind, 'TA_F'] <= 60:\n",
    "            df.loc[ind, 'PA_INSTUNC'] = 2\n",
    "\n",
    "    df['TA_INSTUNC'] = df['TA_INSTUNC'].astype(float)\n",
    "    df['RH_INSTUNC'] = df['RH_INSTUNC'].astype(float)\n",
    "    df['VPD_INSTUNC'] = df['VPD_F']*(num.sqrt(((df['TA_INSTUNC']/df['TA_F'])**2)+((df['RH_INSTUNC']/df['RH'])**2)))\n",
    "    df['PA_INSTUNC'] = df['PA_INSTUNC'].astype(float)\n",
    "    ta = ((df['TA_INSTUNC'] / df['TA_F'])**2)*1000000\n",
    "    le = (df['LE_RANDUNC'] / df['LE_F_MDS'])**2\n",
    "    gpp = ((df['GPP_NT_VUT_REF'] / df['GPP_NT_VUT_50'])**2)*0.001\n",
    "    df['eWUE_INSTUNC'] = df['eWUE'] * (num.sqrt(ta+le+gpp)*18.01/1000)\n",
    "    gpplue = ((df['GPP_NT_VUT_REF'] / df['GPP_NT_VUT_50'])**2)\n",
    "    par = (df['PAR_UNC'] / df['PAR'])**2\n",
    "    df['LUE_INSTUNC'] = df['LUE'] * (num.sqrt(gpplue + par)*1000)\n",
    "    df['PA_F'] = df['PA_F']*10\n",
    "\n",
    "    return df\n",
    "\n",
    "def net_std_calc(df, row_index, window_size, rg_thres=50, ta_thres=2.5, vpd_thres=5.0):\n",
    "    startdate = df['TIMESTAMP'][row_index] - pd.DateOffset(days=window_size)\n",
    "    enddate = df['TIMESTAMP'][row_index] + pd.DateOffset(days=window_size)\n",
    "\n",
    "    mask = (\n",
    "        (df['TIMESTAMP'] >= startdate) & (df['TIMESTAMP'] <= enddate) &\n",
    "        (abs(df['NETRAD'] - df['NETRAD'][row_index]) <= rg_thres) &\n",
    "        (abs(df['TA_F'] - df['TA_F'][row_index]) <= ta_thres) &\n",
    "        (abs(df['VPD_F'] - df['VPD_F'][row_index]) <= vpd_thres)\n",
    "    )\n",
    "    selected_data = df[mask]\n",
    "    if not selected_data.empty:\n",
    "        std_devnet = selected_data['NETRAD'].std()\n",
    "        std_devpar = selected_data['PAR'].std()\n",
    "    else:\n",
    "        std_devnet = num.nan\n",
    "        std_devpar = num.nan\n",
    "    return std_devnet, std_devpar    \n",
    "\n",
    "def netraduncs(df: pd.DataFrame):\n",
    "    uncdf = df[['TIMESTAMP','NETRAD', 'TA_F', 'VPD_F', 'PAR']].copy()\n",
    "    uncdf['TIMESTAMP'] = pd.to_datetime(uncdf['TIMESTAMP'], format= '%Y%m%d')\n",
    "\n",
    "    window_size = 7\n",
    "    std_dev_netvalues = []\n",
    "    std_dev_parvalues = []\n",
    "\n",
    "    for i in range(len(uncdf)):\n",
    "        std_devnet, std_devpar = net_std_calc(uncdf, i, window_size)\n",
    "        std_dev_netvalues.append(std_devnet)\n",
    "        std_dev_parvalues.append(std_devpar)\n",
    "    \n",
    "    return std_dev_netvalues, std_dev_parvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHalfHourAvgDFs(criteria, bin:str):\n",
    "    #convert to df for inner join then join\n",
    "    N = len(criteria)\n",
    "    # print(bin, N)\n",
    "    criteria=pd.DataFrame(criteria, columns=['TIMESTAMP'])\n",
    "    criteria=pd.merge(criteria, halfhourlydf, on='TIMESTAMP')\n",
    "\n",
    "    #creating uncertainties for NETRAD and PAR... computationally expensive, don't need to run every time\n",
    "    # criteria['NETRAD_UNC'], criteria['PAR_UNC'] = netraduncs(criteria)\n",
    "    # criteria = calc_taunc(criteria)\n",
    "\n",
    "    #saving to csv file\n",
    "    # csv = criteria.to_csv(root + '/' + bin + \".csv\")\n",
    "\n",
    "    #calculate EF integrals \n",
    "    #efints(criteria, bin, N)\n",
    "    #statistics before averaging all half hours for graphing\n",
    "    statistic_calcs(criteria, N, bin)\n",
    "    \n",
    "    #average timesteps\n",
    "    criteria['HalfHour']=criteria['TIMESTAMP_START'].str[8:]\n",
    "    criteria=criteria.groupby(['HalfHour']).mean(numeric_only = True)\n",
    "    criteria=criteria.reset_index()\n",
    "\n",
    "    # csv = criteria.to_csv(root + '/' + bin + \"48.csv\")\n",
    "\n",
    "    return criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18436\\2126990640.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#run all the functions above to create stats dfs and dfs with average daily cycle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdiurnal_centroid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdDryLow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'NEE_VUT_REF'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ddl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munitesperday\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# dDryLow= createHalfHourAvgDFs(dDryLow, 'ddl')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# dDryMedium= createHalfHourAvgDFs(dDryMedium, 'ddm')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18436\\1778022660.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(flux, bin, unitesperday)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdiurnal_centroid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflux\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munitesperday\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mdays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUPD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0munitesperday\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mhours\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUPD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mUPD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhours\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mflux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m48\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0munitesperday\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\emmac\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5898\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5899\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5900\u001b[0m         ):\n\u001b[0;32m   5901\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5902\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "#run all the functions above to create stats dfs and dfs with average daily cycle\n",
    "diurnal_centroid(dDryLow['NEE_VUT_REF'], 'ddl', unitesperday=48)\n",
    "\n",
    "# dDryLow= createHalfHourAvgDFs(dDryLow, 'ddl')\n",
    "# dDryMedium= createHalfHourAvgDFs(dDryMedium, 'ddm')\n",
    "# dDryHigh= createHalfHourAvgDFs(dDryHigh, 'ddh')\n",
    "\n",
    "# dWetLow= createHalfHourAvgDFs(dWetLow, 'dwl')\n",
    "# dWetMedium= createHalfHourAvgDFs(dWetMedium, 'dwm')\n",
    "# dWetHigh= createHalfHourAvgDFs(dWetHigh, 'dwh')\n",
    "\n",
    "# wWetLow= createHalfHourAvgDFs(wWetLow, 'wwl')\n",
    "# wWetMedium= createHalfHourAvgDFs(wWetMedium, 'wwm')\n",
    "# wWetHigh= createHalfHourAvgDFs(wWetHigh, 'wwh')\n",
    "\n",
    "# wDryLow= createHalfHourAvgDFs(wDryLow, 'wdl')\n",
    "# wDryMedium= createHalfHourAvgDFs(wDryMedium, 'wdm')\n",
    "# wDryHigh= createHalfHourAvgDFs(wDryHigh, 'wdh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anova things\n",
    "maindfnamelist = ['dDryLow', 'dDryMedium', 'dDryHigh', 'dWetLow', 'dWetMedium', 'dWetHigh', 'wWetLow', 'wWetMedium', 'wWetHigh', 'wDryLow', 'wDryMedium', 'wDryHigh']\n",
    "\n",
    "def anova_tukey(listoflists, dfnamelist):\n",
    "    #listoflists is array with one variable and one stat, example: NEE_INT\n",
    "    #next line has rows = all nee ints for one bin, columns = number of days in each bin\n",
    "    df=pd.DataFrame(listoflists)\n",
    "    #switch to rows = number of days in each bin, columns = bin\n",
    "    df = num.transpose(df)\n",
    "    #add labels for anova\n",
    "    df = df.set_axis(dfnamelist, axis=1, copy=False)\n",
    "    data = [df[col].dropna() for col in df]\n",
    "    f_val, p_val = stats.f_oneway(*data)\n",
    "    df_melt = pd.melt(df.reset_index(), id_vars =['index'], value_vars=dfnamelist)\n",
    "    df_melt.columns = ['index', 'treatments', 'value']\n",
    "    res = stat()\n",
    "    #future warning error is because xfac_var has nans\n",
    "    res.tukey_hsd(df=df_melt, res_var='value', xfac_var='treatments', anova_model='value ~C(treatments)')\n",
    "    return f_val, p_val, res.tukey_summary \n",
    "\n",
    "def anova(listoflists, dfnamelist):\n",
    "    df=pd.DataFrame(listoflists)\n",
    "    df=num.transpose(df)\n",
    "    df=df.set_axis(dfnamelist, axis=1, copy=False)\n",
    "    df_melt=pd.melt(df.reset_index(), id_vars=['index'], value_vars=dfnamelist)\n",
    "    df_melt.columns=['index', 'treatments', 'value']\n",
    "    df_melt['pc']=df_melt['treatments'].str[:4]\n",
    "    df_melt['stress']=df_melt['treatments'].str[4:]\n",
    "    df_melt=df_melt.drop(['index', 'treatments'], axis=1)\n",
    "    df_melt=df_melt.dropna()\n",
    "    df_melt=df_melt.reset_index()\n",
    "\n",
    "    formula = 'value ~ C(pc) * C(stress)'\n",
    "    model=ols(formula, data=df_melt).fit()\n",
    "    anova_table=sm.stats.anova_lm(model, typ=3)\n",
    "    print(anova_table)\n",
    "    print()\n",
    "    print(model.summary())\n",
    "\n",
    "\n",
    "    pc_levels = df_melt['pc'].unique()\n",
    "    stress_levels = df_melt['stress'].unique()\n",
    "\n",
    "    lsmeans = []\n",
    "    for pc_level in pc_levels:\n",
    "        for stress_level in stress_levels:\n",
    "            term = f'C(pc):C(stress)'\n",
    "            lsmean = model.get_robustcov_results().t_test([term])\n",
    "            lsmeans.append((f'PC{pc_level}_Stress{stress_level}', lsmean))\n",
    "\n",
    "    # Print the least squares means\n",
    "    print(\"\\nLeast Squares Means:\")\n",
    "    for lsmean in lsmeans:\n",
    "        print(f\"{lsmean[0]}: {lsmean[1].effect[0]:.4f}\")\n",
    "\n",
    "    # Calculate Least Squares Means (LSMeans)\n",
    "    # tukey_results = pairwise_tukeyhsd(df_melt['value'], df_melt['pc'] + df_melt['stress'])\n",
    "    # lsmeans = pd.DataFrame(tukey_results.meandiffs, columns=['LSMeans'])\n",
    "    # lsmeans['CI_lower'] = tukey_results.confint[:, 0]\n",
    "    # lsmeans['CI_upper'] = tukey_results.confint[:, 1]\n",
    "\n",
    "    # print(lsmeans)\n",
    "    # tukey_results = pairwise_tukeyhsd(df_melt['value'], df_melt['pc'] + df_melt['stress'])\n",
    "    # #print(tukey_results)\n",
    "    # ls_means = tukey_results.meandiffs + df_melt['value'].mean()\n",
    "\n",
    "    # print(ls_means)\n",
    "    # fig=interaction_plot(x=df_melt['pc'], trace=df_melt['stress'], response=df_melt['value'], colors=['#4c061d','#d17a22', '#b4c292'])\n",
    "    # plt.show()   \n",
    "\n",
    "#currently outputting the differences of all the combinations of groups of their least square means, but i want ONLY the least square means\n",
    "anova(h_int, maindfnamelist)\n",
    "# fh, ph, th = anova_tukey(h_int, maindfnamelist)\n",
    "# print(fh)\n",
    "# print(ph)\n",
    "# print(th)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
